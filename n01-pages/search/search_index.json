{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"n01 Oceans and Shelf Seas consortium pages This is a personal documentation site which will be of some benefit to other members of the n01, Oceans and Shelf Seas consortium. This site will host more detailed help and tips on using the codes most frequently employed by members of the consortium and with associated post-processing tasks. Consortium Software Post-processing tips Forcing datasets Personal projects Contributing to the documentation The source for this documentation is publicly available in my Github repository so that anyone can contribute to improve the documentation. Contributions can be in the form of improvements or addtions to the content. Credits This documentation builds on the nemo section that was contributed to the Official ARCHER2 documentation . That section is repeated here under the Consortium Software page. This documentation is intended to supplement the ARCHER2 service documentation produced by EPCC, The University of Edinburgh which should always be consulted as the definitive guide to wider aspects of the service.","title":"Documentation overview"},{"location":"#n01-oceans-and-shelf-seas-consortium-pages","text":"This is a personal documentation site which will be of some benefit to other members of the n01, Oceans and Shelf Seas consortium. This site will host more detailed help and tips on using the codes most frequently employed by members of the consortium and with associated post-processing tasks. Consortium Software Post-processing tips Forcing datasets Personal projects","title":"n01 Oceans and Shelf Seas consortium pages"},{"location":"#contributing-to-the-documentation","text":"The source for this documentation is publicly available in my Github repository so that anyone can contribute to improve the documentation. Contributions can be in the form of improvements or addtions to the content.","title":"Contributing to the documentation"},{"location":"#credits","text":"This documentation builds on the nemo section that was contributed to the Official ARCHER2 documentation . That section is repeated here under the Consortium Software page. This documentation is intended to supplement the ARCHER2 service documentation produced by EPCC, The University of Edinburgh which should always be consulted as the definitive guide to wider aspects of the service.","title":"Credits"},{"location":"forcing/","text":"Forcing datasets This page is under development Consortium Software","title":"Forcing datasets"},{"location":"forcing/#forcing-datasets","text":"This page is under development Consortium Software","title":"Forcing datasets"},{"location":"models/","text":"Consortium Software The following sections provide tips on using some of the more popular codes in actve use my members of the n01 (oceans and shelf seas) consortinum: NEMO","title":"Consortium Software"},{"location":"models/#consortium-software","text":"The following sections provide tips on using some of the more popular codes in actve use my members of the n01 (oceans and shelf seas) consortinum: NEMO","title":"Consortium Software"},{"location":"models/nemo/nemo/","text":"NEMO NEMO (Nucleus for European Modelling of the Ocean) is a state-of-the-art framework for research activities and forecasting services in ocean and climate sciences, developed in a sustainable way by a European consortium. Useful Links The NEMO home page https://www.nemo-ocean.eu NEMO documentation https://forge.ipsl.jussieu.fr/nemo/chrome/site/doc/NEMO/guide/html/NEMO_guide.html NEMO users' area http://forge.ipsl.jussieu.fr/nemo/wiki/Users which includes information on obtaining and downloading the latest source code releases. NEMO is released under a CeCILL license and if freely available to all users on ARCHER2. Using NEMO on ARCHER2 A central install of NEMO is not appropriate for most users of ARCHER2 since many configurations will want to add bespoke code changes. There is, however, a case for providing some central material specific to ARCHER2. This includes: arch files for compiling on ARCHER2; a pre-compiled version of XIOS and guides for running NEMO on ARCHER2. The guides provided here address the running of NEMO as a stand-alone ocean model (albeit with optional sea-ice and external XIOS i/o servers ) and provide links to some of the material held in the shared workspace of the n01 (oceans and shelf seas) consortium. The running of full Earth System Models with NEMO as just one component of a fully interacting, OASIS-coupled, suite is best dealt with by more comprehensive workflow management systems such as the Rose and Cylc set-up used by NCAS. Setting up the correct environment The first point of note is that NEMO will not operate successfully in the default environment on ARCHER2. To be precise, this statement is true for any attempts to run NEMO as part of a MPMD task with external XIOS servers. In attached mode, where no external servers are used and every ocean process acts as an io server, then the default environment can be used to launch NEMO as a SPMD task. Since attached mode is not performant at high core counts it is advisable to standardise on on environment which is suitable for all NEMO applications. This is currently: Either (starting from the default environment): module unload cray-mpich module load craype-network-ucx module load cray-mpich-ucx module load libfabric module load cray-hdf5-parallel module load cray-netcdf-hdf5parallel module load gcc - Or, equivalently: module -s restore /work/n01/shared/acc/n01_modules/ucx_env If your NEMO tasks are failing at start-up (or possibly hanging) then the chances are you are attempting to use the wrong mpich library. Note all investigations to date have focussed on using the Cray compilers. Enabling FCM to compile in parallel with Cray compilers FCM is bundled with both NEMO and XIOS and is used by makenemo and make_xios scripts These scripts accept \u2013j N/-jobs N arguments for parallel builds but the Cray compilers trip up attempting to load modules which have only just been built. It seems to be a timing issue because if the -J option is added to inform the compiler where to look for modules the problem disappears. The -J option should not be necessary because the -I setting is already given and, according to the manual, directories given by -J are searched first followed by those given by -I. The slight difference in priority seems to matter though and parallel builds will fail with the Cray compilers unless the following change is made to: NEMO/r4.0.X/ext/FCM/lib/Fcm/Config.pm (NEMO4 source tree) and (if compiling xios, not everyone needs to do this, see next section) xios-2.5/tools/FCM/lib/Fcm/Config.pm (may need to run make_xios once to unpack this) In both cases change: FC_MODSEARCH => '', # FC flag, specify \"module\" path to FC_MODSEARCH => '-J', # FC flag, specify \"module\" path Compiling XIOS and NEMO It is not necessary for everyone to compile XIOS. A compiled version is available in: /work/n01/shared/acc/xios-2.5 This was compiled with: ./make_xios --prod --arch X86_ARCHER2-Cray --netcdf_lib netcdf4_par --job 16 -full using the: /work/n01/shared/acc/xios-2.5/arch/arch-X86_ARCHER2-Cray.[env,fcm,path] settings The NEMO arch file suggested at the next step will link nemo with the xios libraries contained therein and copies of the xios_server.exe executable can be taken from the xios-2.5/bin directory. It is recommended to take copies of this executable to guard against any possible issues with future updates. NEMO can be compiled with (for example): ./makenemo -n ORCA2_ICE_PISCES_ST -r ORCA2_ICE_PISCES -m X86_ARCHER2-Cray -j 16 once the: /work/n01/shared/acc/arch-X86_ARCHER2-Cray.fcm file has been dropped into the NEMO arch directory. Note this arch file currently sets compiler flags of: -em -s integer32 -s real64 -O1 -hflex_mp=intolerant Small gains at higher optimisation levels are offset by much longer compile times and an inability to attain restartability and reproducibility passes in some of the standard SETTE tests. Future releases of NEMO (post r4.0.4) will contain this arch file for ARCHER2. Users of versions 4.0.4 and earlier will have to manually add this file. Building a run script Note The following 5 sections describe an evolving approach to running NEMO on ARCHER2 that culminated in the recommended method described in the last section: Running heterogeneous jobs . If you are not interested in the details of how this solution was reached, skip straight to that final section . Most NEMO applications will want to run in detached mode with separate XIOS servers. Remember this is only currently possible using the cray-mpich-ucx module. MPMD jobs are more complex to set-up than on ARCHER; but also more versatile since executables can be mixed on a node. Generally, xios_servers will need to be more lightly packed than NEMO cores because: xios servers have less consistent memory requirements than the ocean cores They generally require more memory and their needs can spike depending on output interval or experimental needs There are far fewer of them than ocean cores so a pragmatic solution might be to assign each xios_server an entire NUMA region of its own It also makes sense to avoid concentrating too many xios_servers on any particular node Very large models may require xios_servers to occupy more than one NUMA (TBD) All this can be achieved using the: \u2013cpu-bind=map_cpu:<cpu map> option to srun but it is tedious to construct cpu maps by hand. The: /work/n01/shared/acc/mkslurm script will construct a basic run script using supplied packing arguments. The mkslurm script usage: mkslurm [-S num_servers] [-s server_spacing] [-m max_servers_per_node] [-C num_clients] [-c client_spacing] [-t time_limit] [-a account] [-j job_name] It is recommended to take your own copy and set defaults for most of these arguments For example, to run with 4 xios servers (a maximum of 2 per node), each with sole occupancy of a 16-core NUMA region and 96 ocean cores, spaced with an idle core in between each, use: ./mkslurm -S 4 -s 16 -m 2 -C 96 -c 2 > myscript.slurm This will report (to stderr) that 2 nodes are needed with 100 active cores spread over 256 cores. It will also echo the equivalent full command (to stderr) to show the defaults used for those arguments not given, i.e.: Running: mkslurm -S 4 -s 16 -m 4 -C 96 -c 2 -t 00:10:00 -a n01 -j nemo_test nodes needed= 2 (256) cores to be used= 100 (256) The slurm script produced is shown in the next section Running parallel NEMO jobs The script produced is: #!/bin/bash #SBATCH --job-name=nemo_test #SBATCH --time=00:10:00 #SBATCH --nodes=2 #SBATCH --ntasks=100 #SBATCH --account=n01 #SBATCH --partition=standard #SBATCH --qos=standard export OMP_NUM_THREADS=1 module restore /work/n01/shared/acc/n01_modules/ucx_env # cat > myscript_wrapper2.sh << EOFB #!/bin/ksh # set -A map ./xios_server.exe ./nemo exec_map=( 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ) # exec \\${map[\\${exec_map[\\$SLURM_PROCID]}]} ## EOFB chmod u+x ./myscript_wrapper2.sh # srun --mem-bind=local --cpu-bind=v,map_cpu:00,0x10,0x20,0x22,0x24,0x26,0x28,0x2a,0x2c,0x2e,0x30,0x32,0x34,0x36,0x38,0x3a,0x3c,0x3e,0x40,0x42,0x44,0x46,0x48,0x4a,0x4c,0x4e,0x50,0x52,0x54,0x56,0x58,0x5a,0x5c,0x5e,0x60,0x62,0x64,0x66,0x68,0x6a,0x6c,0x6e,0x70,0x72,0x74,0x76,0x78,0x7a,0x7c,0x7e,00,0x10,0x20,0x22,0x24,0x26,0x28,0x2a,0x2c,0x2e,0x30,0x32,0x34,0x36,0x38,0x3a,0x3c,0x3e,0x40,0x42,0x44,0x46,0x48,0x4a,0x4c,0x4e,0x50,0x52,0x54,0x56,0x58,0x5a,0x5c,0x5e,0x60,0x62,0x64,0x66,0x68,0x6a,0x6c,0x6e,0x70,0x72,0x74,0x76,0x78,0x7a,0x7c,0x7e, ./myscript_wrapper2.sh which will run the desired MPMD job providing the xios_server.exe and nemo executables are in the directory that this script is submitted from. The exec_map array shows the position of each executable in the rank list (0=xios_server.exe, 1=nemo). The cpu map gives the hexadecimal number of the core that will run that executable. For larger core counts the cpu_map can be limited to a single node map which will be cycled through as many times as necessary until all the tasks are mapped. Some preliminary tests This table shows the results of a repeated 60 day simulation of the ORCA2_ICE_PISCES, SETTE configuration using various core counts and packing strategies: Alternative placement strategies It is clear from the previous results that fully populating an ARCHER2 node is unlikely to provide the optimal performance for any codes with moderate memory bandwidth requirements. The regular packing strategy offered by mkslurm does not allow experimentation with less wasteful packing strategies than half-population though. There may be a case, for example, for just leaving every 1 in 4 cores idle, or every 1 in 8, or even fewer idle cores per node. The mkslurm_alt script (/work/n01/shared/acc/mkslurm_alt) provides a method of generating cpu-bind maps for exploring these strategies. The script assumes no change in the packing strategy for the servers but the core spacing argument (-c) for the ocean cores is replaced by a -g option representing the frequency of a gap in the, otherwise tightly-packed, ocean cores. A -v option has also been introduced to provide a human-readable indication of the core usage. I.e.: usage: mkslurm_alt [-S num_servers] [-s server_spacing] [-m max_servers_per_node] [-C num_clients] [-g client_gap_interval] [-t time_limit] [-a account] [-j job_name] [-v] Tests with alternative packing strategies Preliminary tests have been conducted with the ORCA2_ICE_PISCES SETTE test case. This is a relatively small test case that will fit onto single node. It is also small enough to perform well in attached mode. First some baseline tests in attached mode. For 32 and 64 core tests, these are equivalent to running a mkslurm-generated script with -S 0 and -C 32 -c 4 and -C 64 -c 2, respectively: Previous tests used 4 I/O servers each occupying a single NUMA. For this size model, 2 servers occupying half a NUMA each will suffice. That leaves 112 cores with which to try different packing strategies. Is it possible to match or better this elapsed time on a single node including external I/O servers? -Yes! -but not with an obvious gap frequency: And activating land suppression can reduce times further: The optimal two-node solution is also shown (this is quicker but the one node solution is cheaper). Running heterogeneous jobs There is a critical limitation to the techniques described so far for placing tasks on the ARCHER2 nodes. That is that the cpu map generated by the mkslurm and mkslurm_alt scripts is only applied as expected if the layout is identical on every node. In general, this is not the case for larger NEMO configurations because: We may only want server processes (possibly mixed with ocean processes) on an initial subset of nodes followed by a set of fully utilised (albeit, possibly, with regular gaps) nodes of only ocean processes followed (possibly) by a final node with fewer ocean processes To exercise precise control over the placement in each of these parts will require different cpu maps or masks. This can be achieved by configuring each part as a component of a heterogeneous job-pack. The basic principle is explained here: Heterogeneous jobs A python-based extension of the mkslurm_alt functionality has been provided to automatically generate such job-scripts. The script is located here: /work/n01/shared/malmans/mkslurm_hetjob The script generates a 'job-pack' (heterogeneous job in modern Slurm parlance) of up to three parts, corresponding to the possible three components and generates the correct cpu placement for each part. The jobs have separate entries in the queue but can only be run or cancelled as a pack and all run within the same MPI communicator when they start. Arguments to the script match those of mkslurm_alt, i.e.: usage: mkslurm_hetjob [-h] [-S S] [-s S] [-m M] [-C C] [-g G] [-N N] [-t T] [-a A] [-j J] [-v] Python version of mkslurm_alt using HetJob. Server placement and spacing remains as mkslurm but clients are always tightly packed with a gap left every \"NC_GAP\" cores where NC_GAP can be given by the -g argument. values of 4, 8 or 16 are recommended. optional arguments: -h, --help show this help message and exit -S S num_servers (default: 4) -s S server_spacing (default: 8) -m M max_servers_per_node (default: 2) -C C num_clients (default: 28) -g G client_gap_interval (default: 4) -N N ncores_per_node (default: 128) -t T time_limit (default: 00:10:00) -a A account (default: n01) -j J job_name (default: nemo_test) -v show human readable hetjobs (default: False) The use of this script to generate job scripts for most nemo, ocean-only runs is recommended. Note a tightly-packed placement with no gaps amongst the ocean processes can be generated using a client gap interval greater than the number of clients. This script has been used to explore the different placement strategies with a larger configuration based on eORCA025. In all cases, 8 XIOS servers were used, each with sole occupancy of a 16-core NUMA and a maximum of 2 servers per node. The rest of the initial 4 nodes (and any subsequent ocean core-only nodes) were filled with ocean cores at various packing densities (from tightly packed to half-populated). A summary of the results are shown below. The limit of scalability for this problem size lies around 1500 cores. One interesting aspect is that the cost, in terms of node hours, remains fairly flat up to a thousand processes and the choice of gap placement makes much less difference as the individual domains shrink. It looks as if, so long as you avoid inappropriately high numbers of processors, choosing the wrong placement won't waste your allocation but may waste your time. Note This information is based on experience during early user testing and is subject to change","title":"NEMO"},{"location":"models/nemo/nemo/#nemo","text":"NEMO (Nucleus for European Modelling of the Ocean) is a state-of-the-art framework for research activities and forecasting services in ocean and climate sciences, developed in a sustainable way by a European consortium.","title":"NEMO"},{"location":"models/nemo/nemo/#useful-links","text":"The NEMO home page https://www.nemo-ocean.eu NEMO documentation https://forge.ipsl.jussieu.fr/nemo/chrome/site/doc/NEMO/guide/html/NEMO_guide.html NEMO users' area http://forge.ipsl.jussieu.fr/nemo/wiki/Users which includes information on obtaining and downloading the latest source code releases. NEMO is released under a CeCILL license and if freely available to all users on ARCHER2.","title":"Useful Links"},{"location":"models/nemo/nemo/#using-nemo-on-archer2","text":"A central install of NEMO is not appropriate for most users of ARCHER2 since many configurations will want to add bespoke code changes. There is, however, a case for providing some central material specific to ARCHER2. This includes: arch files for compiling on ARCHER2; a pre-compiled version of XIOS and guides for running NEMO on ARCHER2. The guides provided here address the running of NEMO as a stand-alone ocean model (albeit with optional sea-ice and external XIOS i/o servers ) and provide links to some of the material held in the shared workspace of the n01 (oceans and shelf seas) consortium. The running of full Earth System Models with NEMO as just one component of a fully interacting, OASIS-coupled, suite is best dealt with by more comprehensive workflow management systems such as the Rose and Cylc set-up used by NCAS.","title":"Using NEMO on ARCHER2"},{"location":"models/nemo/nemo/#setting-up-the-correct-environment","text":"The first point of note is that NEMO will not operate successfully in the default environment on ARCHER2. To be precise, this statement is true for any attempts to run NEMO as part of a MPMD task with external XIOS servers. In attached mode, where no external servers are used and every ocean process acts as an io server, then the default environment can be used to launch NEMO as a SPMD task. Since attached mode is not performant at high core counts it is advisable to standardise on on environment which is suitable for all NEMO applications. This is currently: Either (starting from the default environment): module unload cray-mpich module load craype-network-ucx module load cray-mpich-ucx module load libfabric module load cray-hdf5-parallel module load cray-netcdf-hdf5parallel module load gcc - Or, equivalently: module -s restore /work/n01/shared/acc/n01_modules/ucx_env If your NEMO tasks are failing at start-up (or possibly hanging) then the chances are you are attempting to use the wrong mpich library. Note all investigations to date have focussed on using the Cray compilers.","title":"Setting up the correct environment"},{"location":"models/nemo/nemo/#enabling-fcm-to-compile-in-parallel-with-cray-compilers","text":"FCM is bundled with both NEMO and XIOS and is used by makenemo and make_xios scripts These scripts accept \u2013j N/-jobs N arguments for parallel builds but the Cray compilers trip up attempting to load modules which have only just been built. It seems to be a timing issue because if the -J option is added to inform the compiler where to look for modules the problem disappears. The -J option should not be necessary because the -I setting is already given and, according to the manual, directories given by -J are searched first followed by those given by -I. The slight difference in priority seems to matter though and parallel builds will fail with the Cray compilers unless the following change is made to: NEMO/r4.0.X/ext/FCM/lib/Fcm/Config.pm (NEMO4 source tree) and (if compiling xios, not everyone needs to do this, see next section) xios-2.5/tools/FCM/lib/Fcm/Config.pm (may need to run make_xios once to unpack this) In both cases change: FC_MODSEARCH => '', # FC flag, specify \"module\" path to FC_MODSEARCH => '-J', # FC flag, specify \"module\" path","title":"Enabling FCM to compile in parallel with Cray compilers"},{"location":"models/nemo/nemo/#compiling-xios-and-nemo","text":"It is not necessary for everyone to compile XIOS. A compiled version is available in: /work/n01/shared/acc/xios-2.5 This was compiled with: ./make_xios --prod --arch X86_ARCHER2-Cray --netcdf_lib netcdf4_par --job 16 -full using the: /work/n01/shared/acc/xios-2.5/arch/arch-X86_ARCHER2-Cray.[env,fcm,path] settings The NEMO arch file suggested at the next step will link nemo with the xios libraries contained therein and copies of the xios_server.exe executable can be taken from the xios-2.5/bin directory. It is recommended to take copies of this executable to guard against any possible issues with future updates. NEMO can be compiled with (for example): ./makenemo -n ORCA2_ICE_PISCES_ST -r ORCA2_ICE_PISCES -m X86_ARCHER2-Cray -j 16 once the: /work/n01/shared/acc/arch-X86_ARCHER2-Cray.fcm file has been dropped into the NEMO arch directory. Note this arch file currently sets compiler flags of: -em -s integer32 -s real64 -O1 -hflex_mp=intolerant Small gains at higher optimisation levels are offset by much longer compile times and an inability to attain restartability and reproducibility passes in some of the standard SETTE tests. Future releases of NEMO (post r4.0.4) will contain this arch file for ARCHER2. Users of versions 4.0.4 and earlier will have to manually add this file.","title":"Compiling XIOS and NEMO"},{"location":"models/nemo/nemo/#building-a-run-script","text":"Note The following 5 sections describe an evolving approach to running NEMO on ARCHER2 that culminated in the recommended method described in the last section: Running heterogeneous jobs . If you are not interested in the details of how this solution was reached, skip straight to that final section . Most NEMO applications will want to run in detached mode with separate XIOS servers. Remember this is only currently possible using the cray-mpich-ucx module. MPMD jobs are more complex to set-up than on ARCHER; but also more versatile since executables can be mixed on a node. Generally, xios_servers will need to be more lightly packed than NEMO cores because: xios servers have less consistent memory requirements than the ocean cores They generally require more memory and their needs can spike depending on output interval or experimental needs There are far fewer of them than ocean cores so a pragmatic solution might be to assign each xios_server an entire NUMA region of its own It also makes sense to avoid concentrating too many xios_servers on any particular node Very large models may require xios_servers to occupy more than one NUMA (TBD) All this can be achieved using the: \u2013cpu-bind=map_cpu:<cpu map> option to srun but it is tedious to construct cpu maps by hand. The: /work/n01/shared/acc/mkslurm script will construct a basic run script using supplied packing arguments.","title":"Building a run script"},{"location":"models/nemo/nemo/#the-mkslurm-script","text":"usage: mkslurm [-S num_servers] [-s server_spacing] [-m max_servers_per_node] [-C num_clients] [-c client_spacing] [-t time_limit] [-a account] [-j job_name] It is recommended to take your own copy and set defaults for most of these arguments For example, to run with 4 xios servers (a maximum of 2 per node), each with sole occupancy of a 16-core NUMA region and 96 ocean cores, spaced with an idle core in between each, use: ./mkslurm -S 4 -s 16 -m 2 -C 96 -c 2 > myscript.slurm This will report (to stderr) that 2 nodes are needed with 100 active cores spread over 256 cores. It will also echo the equivalent full command (to stderr) to show the defaults used for those arguments not given, i.e.: Running: mkslurm -S 4 -s 16 -m 4 -C 96 -c 2 -t 00:10:00 -a n01 -j nemo_test nodes needed= 2 (256) cores to be used= 100 (256) The slurm script produced is shown in the next section","title":"The mkslurm script"},{"location":"models/nemo/nemo/#running-parallel-nemo-jobs","text":"The script produced is: #!/bin/bash #SBATCH --job-name=nemo_test #SBATCH --time=00:10:00 #SBATCH --nodes=2 #SBATCH --ntasks=100 #SBATCH --account=n01 #SBATCH --partition=standard #SBATCH --qos=standard export OMP_NUM_THREADS=1 module restore /work/n01/shared/acc/n01_modules/ucx_env # cat > myscript_wrapper2.sh << EOFB #!/bin/ksh # set -A map ./xios_server.exe ./nemo exec_map=( 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ) # exec \\${map[\\${exec_map[\\$SLURM_PROCID]}]} ## EOFB chmod u+x ./myscript_wrapper2.sh # srun --mem-bind=local --cpu-bind=v,map_cpu:00,0x10,0x20,0x22,0x24,0x26,0x28,0x2a,0x2c,0x2e,0x30,0x32,0x34,0x36,0x38,0x3a,0x3c,0x3e,0x40,0x42,0x44,0x46,0x48,0x4a,0x4c,0x4e,0x50,0x52,0x54,0x56,0x58,0x5a,0x5c,0x5e,0x60,0x62,0x64,0x66,0x68,0x6a,0x6c,0x6e,0x70,0x72,0x74,0x76,0x78,0x7a,0x7c,0x7e,00,0x10,0x20,0x22,0x24,0x26,0x28,0x2a,0x2c,0x2e,0x30,0x32,0x34,0x36,0x38,0x3a,0x3c,0x3e,0x40,0x42,0x44,0x46,0x48,0x4a,0x4c,0x4e,0x50,0x52,0x54,0x56,0x58,0x5a,0x5c,0x5e,0x60,0x62,0x64,0x66,0x68,0x6a,0x6c,0x6e,0x70,0x72,0x74,0x76,0x78,0x7a,0x7c,0x7e, ./myscript_wrapper2.sh which will run the desired MPMD job providing the xios_server.exe and nemo executables are in the directory that this script is submitted from. The exec_map array shows the position of each executable in the rank list (0=xios_server.exe, 1=nemo). The cpu map gives the hexadecimal number of the core that will run that executable. For larger core counts the cpu_map can be limited to a single node map which will be cycled through as many times as necessary until all the tasks are mapped.","title":"Running parallel NEMO jobs"},{"location":"models/nemo/nemo/#some-preliminary-tests","text":"This table shows the results of a repeated 60 day simulation of the ORCA2_ICE_PISCES, SETTE configuration using various core counts and packing strategies:","title":"Some preliminary tests"},{"location":"models/nemo/nemo/#alternative-placement-strategies","text":"It is clear from the previous results that fully populating an ARCHER2 node is unlikely to provide the optimal performance for any codes with moderate memory bandwidth requirements. The regular packing strategy offered by mkslurm does not allow experimentation with less wasteful packing strategies than half-population though. There may be a case, for example, for just leaving every 1 in 4 cores idle, or every 1 in 8, or even fewer idle cores per node. The mkslurm_alt script (/work/n01/shared/acc/mkslurm_alt) provides a method of generating cpu-bind maps for exploring these strategies. The script assumes no change in the packing strategy for the servers but the core spacing argument (-c) for the ocean cores is replaced by a -g option representing the frequency of a gap in the, otherwise tightly-packed, ocean cores. A -v option has also been introduced to provide a human-readable indication of the core usage. I.e.: usage: mkslurm_alt [-S num_servers] [-s server_spacing] [-m max_servers_per_node] [-C num_clients] [-g client_gap_interval] [-t time_limit] [-a account] [-j job_name] [-v]","title":"Alternative placement strategies"},{"location":"models/nemo/nemo/#tests-with-alternative-packing-strategies","text":"Preliminary tests have been conducted with the ORCA2_ICE_PISCES SETTE test case. This is a relatively small test case that will fit onto single node. It is also small enough to perform well in attached mode. First some baseline tests in attached mode. For 32 and 64 core tests, these are equivalent to running a mkslurm-generated script with -S 0 and -C 32 -c 4 and -C 64 -c 2, respectively: Previous tests used 4 I/O servers each occupying a single NUMA. For this size model, 2 servers occupying half a NUMA each will suffice. That leaves 112 cores with which to try different packing strategies. Is it possible to match or better this elapsed time on a single node including external I/O servers? -Yes! -but not with an obvious gap frequency: And activating land suppression can reduce times further: The optimal two-node solution is also shown (this is quicker but the one node solution is cheaper).","title":"Tests with alternative packing strategies"},{"location":"models/nemo/nemo/#running-heterogeneous-jobs","text":"There is a critical limitation to the techniques described so far for placing tasks on the ARCHER2 nodes. That is that the cpu map generated by the mkslurm and mkslurm_alt scripts is only applied as expected if the layout is identical on every node. In general, this is not the case for larger NEMO configurations because: We may only want server processes (possibly mixed with ocean processes) on an initial subset of nodes followed by a set of fully utilised (albeit, possibly, with regular gaps) nodes of only ocean processes followed (possibly) by a final node with fewer ocean processes To exercise precise control over the placement in each of these parts will require different cpu maps or masks. This can be achieved by configuring each part as a component of a heterogeneous job-pack. The basic principle is explained here: Heterogeneous jobs A python-based extension of the mkslurm_alt functionality has been provided to automatically generate such job-scripts. The script is located here: /work/n01/shared/malmans/mkslurm_hetjob The script generates a 'job-pack' (heterogeneous job in modern Slurm parlance) of up to three parts, corresponding to the possible three components and generates the correct cpu placement for each part. The jobs have separate entries in the queue but can only be run or cancelled as a pack and all run within the same MPI communicator when they start. Arguments to the script match those of mkslurm_alt, i.e.: usage: mkslurm_hetjob [-h] [-S S] [-s S] [-m M] [-C C] [-g G] [-N N] [-t T] [-a A] [-j J] [-v] Python version of mkslurm_alt using HetJob. Server placement and spacing remains as mkslurm but clients are always tightly packed with a gap left every \"NC_GAP\" cores where NC_GAP can be given by the -g argument. values of 4, 8 or 16 are recommended. optional arguments: -h, --help show this help message and exit -S S num_servers (default: 4) -s S server_spacing (default: 8) -m M max_servers_per_node (default: 2) -C C num_clients (default: 28) -g G client_gap_interval (default: 4) -N N ncores_per_node (default: 128) -t T time_limit (default: 00:10:00) -a A account (default: n01) -j J job_name (default: nemo_test) -v show human readable hetjobs (default: False) The use of this script to generate job scripts for most nemo, ocean-only runs is recommended. Note a tightly-packed placement with no gaps amongst the ocean processes can be generated using a client gap interval greater than the number of clients. This script has been used to explore the different placement strategies with a larger configuration based on eORCA025. In all cases, 8 XIOS servers were used, each with sole occupancy of a 16-core NUMA and a maximum of 2 servers per node. The rest of the initial 4 nodes (and any subsequent ocean core-only nodes) were filled with ocean cores at various packing densities (from tightly packed to half-populated). A summary of the results are shown below. The limit of scalability for this problem size lies around 1500 cores. One interesting aspect is that the cost, in terms of node hours, remains fairly flat up to a thousand processes and the choice of gap placement makes much less difference as the individual domains shrink. It looks as if, so long as you avoid inappropriately high numbers of processors, choosing the wrong placement won't waste your allocation but may waste your time. Note This information is based on experience during early user testing and is subject to change","title":"Running heterogeneous jobs"},{"location":"nemo_studio/","text":"Personal projects NemoSIM A Scriptable Image Maker for NEMO output is a personal project that I use frequently to provide both quick look and high-end graphics images from NEMO output. It is ideally suited for producing frames for animation. Video production on ARCHER2 Some ideas for producing animated output wholly on the ARCHER2 platform Consortium Software","title":"Personal projects"},{"location":"nemo_studio/#personal-projects","text":"NemoSIM A Scriptable Image Maker for NEMO output is a personal project that I use frequently to provide both quick look and high-end graphics images from NEMO output. It is ideally suited for producing frames for animation. Video production on ARCHER2 Some ideas for producing animated output wholly on the ARCHER2 platform Consortium Software","title":"Personal projects"},{"location":"nemo_studio/movies/","text":"A first script for producing animation frames on ARCHER2 My NEMO Scriptable Image Maker ( nemosim ) runs on ARCHER2. A typical image such as: was produced with this command: /work/n01/n01/acc/TOOLS/IM/NemoSim/nemosim -d tos -k 1 -bcoord \\ -bathy /work/n01/n01/acc/NEMO/eORCA1/eORCA_R1_zps_domcfg.nc \\ -usemap /work/n01/n01/acc/TOOLS/IM/NemoSim/eORCA1_600x300.nc \\ -limits -1.8 30.0 -r 600 300 -no_offset -sw 0. -90. -ne 360. 90. \\ -np 8 -d2 siconc -overlay \\ -f 1933/eORCA1_MED_UKESM_y1933m05_grid_T.nc \\ -f2 1933/eORCA1_MED_UKESM_y1933m05_icemod.nc \\ -o /work/n01/n01/acc/NOCSMULTITASK/frames/out_3011.png -dateclk 35 40 65 15051933 which looks daunting but is easily generated in a script and, when run in parallel on an ARCHER2 node, can produce thousands of frames in mere minutes. For example: #!/bin/bash #SBATCH --qos=standard #SBATCH --job-name=mfra_1m #SBATCH --time=00:45:00 #SBATCH --nodes=1 #SBATCH --ntasks=12 #SBATCH --ntasks-per-node=12 #SBATCH --cpus-per-task=8 #SBATCH --account=n01 #SBATCH --partition=standard # Created by: mkslurm -S 0 -s 0 -m 2 -C 12 -c 8 -t 00:10:00 -a n01 -j mcnk_1m module -s restore /work/n01/shared/acc/n01_modules/ucx_env export OMP_NUM_THREADS=8 export OMP_PLACES=cores export CRAY_OMP_CHECK_AFFINITY=TRUE export PATH=/work/n01/n01/acc/TOOLS/bin:$PATH export LD_LIBRARY_PATH=/work/n01/n01/acc/TOOLS/lib:/work/n01/n01/acc/TOOLS/IM/lib:$LD_LIBRARY_PATH # tskdir=`pwd` dstdir=`pwd`/frames dtadir=/work/n01/n01/acc/NEMO/OUT_eORCA1/A005/monthly ystart=1850 yend=2100 tsklst=$dstdir/tasks.conf bathyfile=/work/n01/n01/acc/NEMO/eORCA1/eORCA_R1_zps_domcfg.nc mapfile=/work/n01/n01/acc/TOOLS/IM/NemoSim/eORCA1_600x300.nc basecmd=\"/work/n01/n01/acc/TOOLS/IM/NemoSim/nemosim -d tos -k 1 -bcoord -bathy $bathyfile -usemap $mapfile -limits -1.8 30.0 -r 600 300 -no_offset -sw 0. -90. -ne 360. 90. -np $OMP_NUM_THREADS -d2 siconc -overlay \" # cd $dtadir # nf=0 for y in `seq $ystart 1 $yend` do if [ -f $tsklst ] ; then rm $tsklst ; fi n=0 for m in 01 02 03 04 05 06 07 08 09 10 11 12 do for f in `ls -1 ${y}/e*_*y${y}m${m}_grid_T.nc` do f2=${f/grid_T/icemod} if [ -f $f ] && [ -f $f2 ] ; then fnf=`printf \"%4.4d\" $nf` ff=${dstdir}/out_${fnf}.png echo $n $basecmd -f $f -f2 $f2 -o $ff -dateclk 35 40 65 15${m}${y} >> $tsklst n=$(( $n + 1 )) nf=$(( $nf + 1 )) fi done done if [ -f $tsklst ] ; then nt=`wc -l $tsklst | sed -e 's/ .*//'` else nt=0 fi if test $nt -gt 0 ; then srun --ntasks=$nt --mem-bind=local --cpu-bind=v,map_cpu:00,0x8,0x10,0x18,0x20,0x28,0x30,0x38,0x40,0x48,0x50,0x58 --multi-prog $tsklst fi done # cd $tskdir And for compiling these frames into a mp4 movie: ffmpeg -i frames/out_%04d.png -vcodec mpeg4 -vb 4M eorca1.mp4","title":"A first script for producing animation frames on ARCHER2"},{"location":"nemo_studio/movies/#a-first-script-for-producing-animation-frames-on-archer2","text":"My NEMO Scriptable Image Maker ( nemosim ) runs on ARCHER2. A typical image such as: was produced with this command: /work/n01/n01/acc/TOOLS/IM/NemoSim/nemosim -d tos -k 1 -bcoord \\ -bathy /work/n01/n01/acc/NEMO/eORCA1/eORCA_R1_zps_domcfg.nc \\ -usemap /work/n01/n01/acc/TOOLS/IM/NemoSim/eORCA1_600x300.nc \\ -limits -1.8 30.0 -r 600 300 -no_offset -sw 0. -90. -ne 360. 90. \\ -np 8 -d2 siconc -overlay \\ -f 1933/eORCA1_MED_UKESM_y1933m05_grid_T.nc \\ -f2 1933/eORCA1_MED_UKESM_y1933m05_icemod.nc \\ -o /work/n01/n01/acc/NOCSMULTITASK/frames/out_3011.png -dateclk 35 40 65 15051933 which looks daunting but is easily generated in a script and, when run in parallel on an ARCHER2 node, can produce thousands of frames in mere minutes. For example: #!/bin/bash #SBATCH --qos=standard #SBATCH --job-name=mfra_1m #SBATCH --time=00:45:00 #SBATCH --nodes=1 #SBATCH --ntasks=12 #SBATCH --ntasks-per-node=12 #SBATCH --cpus-per-task=8 #SBATCH --account=n01 #SBATCH --partition=standard # Created by: mkslurm -S 0 -s 0 -m 2 -C 12 -c 8 -t 00:10:00 -a n01 -j mcnk_1m module -s restore /work/n01/shared/acc/n01_modules/ucx_env export OMP_NUM_THREADS=8 export OMP_PLACES=cores export CRAY_OMP_CHECK_AFFINITY=TRUE export PATH=/work/n01/n01/acc/TOOLS/bin:$PATH export LD_LIBRARY_PATH=/work/n01/n01/acc/TOOLS/lib:/work/n01/n01/acc/TOOLS/IM/lib:$LD_LIBRARY_PATH # tskdir=`pwd` dstdir=`pwd`/frames dtadir=/work/n01/n01/acc/NEMO/OUT_eORCA1/A005/monthly ystart=1850 yend=2100 tsklst=$dstdir/tasks.conf bathyfile=/work/n01/n01/acc/NEMO/eORCA1/eORCA_R1_zps_domcfg.nc mapfile=/work/n01/n01/acc/TOOLS/IM/NemoSim/eORCA1_600x300.nc basecmd=\"/work/n01/n01/acc/TOOLS/IM/NemoSim/nemosim -d tos -k 1 -bcoord -bathy $bathyfile -usemap $mapfile -limits -1.8 30.0 -r 600 300 -no_offset -sw 0. -90. -ne 360. 90. -np $OMP_NUM_THREADS -d2 siconc -overlay \" # cd $dtadir # nf=0 for y in `seq $ystart 1 $yend` do if [ -f $tsklst ] ; then rm $tsklst ; fi n=0 for m in 01 02 03 04 05 06 07 08 09 10 11 12 do for f in `ls -1 ${y}/e*_*y${y}m${m}_grid_T.nc` do f2=${f/grid_T/icemod} if [ -f $f ] && [ -f $f2 ] ; then fnf=`printf \"%4.4d\" $nf` ff=${dstdir}/out_${fnf}.png echo $n $basecmd -f $f -f2 $f2 -o $ff -dateclk 35 40 65 15${m}${y} >> $tsklst n=$(( $n + 1 )) nf=$(( $nf + 1 )) fi done done if [ -f $tsklst ] ; then nt=`wc -l $tsklst | sed -e 's/ .*//'` else nt=0 fi if test $nt -gt 0 ; then srun --ntasks=$nt --mem-bind=local --cpu-bind=v,map_cpu:00,0x8,0x10,0x18,0x20,0x28,0x30,0x38,0x40,0x48,0x50,0x58 --multi-prog $tsklst fi done # cd $tskdir And for compiling these frames into a mp4 movie: ffmpeg -i frames/out_%04d.png -vcodec mpeg4 -vb 4M eorca1.mp4","title":"A first script for producing animation frames on ARCHER2"},{"location":"postproc/","text":"Post-processing tips ARCHER2 has 128-core, exclusive use nodes but no serial nodes for post-processing How do we make best use of the nodes for serial tasks? E.g.: Compressing and chunking \u2018one_file\u2019 output Rebuilding and chunking \u2018multiple_file\u2019 output What is the best way to run multiple, small-scale parallel tasks? E.g.: Running reproducibility tests Running ensembles of small to medium configurations Top","title":"Post-processing tips"},{"location":"postproc/#post-processing-tips","text":"ARCHER2 has 128-core, exclusive use nodes but no serial nodes for post-processing How do we make best use of the nodes for serial tasks? E.g.: Compressing and chunking \u2018one_file\u2019 output Rebuilding and chunking \u2018multiple_file\u2019 output What is the best way to run multiple, small-scale parallel tasks? E.g.: Running reproducibility tests Running ensembles of small to medium configurations Top","title":"Post-processing tips"},{"location":"postproc/ncks_example/","text":"Running multiple ncks commands simultaneously For small to medium sized NEMO configurations the 'one_file' option with XIOS is a convenient way to avoid creating too many output files. However, since the XIOS servers use NetCDF's parallel I/O capabilities to write to the single files, this precludes sensible chunking and compression settings. A recommended post-processing step is to chunk and compress the output files to help preserve the consortium's limited disk space. The standard ncks tool will do this well enough but a single batch job may have generated many files that need to be processed. Running the ncks utility serially on the output will take too long and is a very poor use of a 128-core node. Take the example of an eORCA1 run with biogeochemistry which can generate 30 years worth of annual and monthly mean datasets in a 10 hour run. In the case of the monthly mean files there are 9 files per month ranging from small (poleward transport diagnostics) to very large (biogeochemical diagnostics). One sensible way to process these is in a triple-nested loop over year then file-type then month. That way 12, roughly equal files can be processed simultaneously and each member of the 12 should complete is a similar time. Now a single ncks command would be something like: ncks --no_abc --cnk_dmn x,64 --cnk_dmn y,64 --cnk_dmn deptht,1 --cnk_dmn time_counter,1 --dfl_lvl 1 \\ eORCA1_MED_UKESM_1m_18500101_18791230_ptrc_T_185001-185001.nc \\ $out_dir/1850/eORCA1_MED_UKESM_y1850m01_ptrc_T.nc where the opportunity has also been taken to prune redundant parts of the filename to create a more concise output filename. To run 12 copies of a similar command simultaneously on a node requires two simple steps: First, create a task-list file of the 12 commands preceded by a zero-based task number, e.g.: cat tasks.conf 0 ncks --no_abc --cnk_dmn x,64 --cnk_dmn y,64 --cnk_dmn deptht,1 --cnk_dmn time_counter,1 --dfl_lvl 1 \\ eORCA1_MED_UKESM_1m_18500101_18791230_ptrc_T_185001-185001.nc \\ $out_dir/1850/eORCA1_MED_UKESM_y1850m01_ptrc_T.nc 1 ncks --no_abc --cnk_dmn x,64 --cnk_dmn y,64 --cnk_dmn deptht,1 --cnk_dmn time_counter,1 --dfl_lvl 1 \\ eORCA1_MED_UKESM_1m_18500101_18791230_ptrc_T_185002-185002.nc \\ $out_dir/1850/eORCA1_MED_UKESM_y1850m02_ptrc_T.nc 2 ncks --no_abc --cnk_dmn x,64 --cnk_dmn y,64 --cnk_dmn deptht,1 --cnk_dmn time_counter,1 --dfl_lvl 1 \\ eORCA1_MED_UKESM_1m_18500101_18791230_ptrc_T_185003-185003.nc \\ $out_dir/1850/eORCA1_MED_UKESM_y1850m03_ptrc_T.nc . . . 10 ncks --no_abc --cnk_dmn x,64 --cnk_dmn y,64 --cnk_dmn deptht,1 --cnk_dmn time_counter,1 --dfl_lvl 1 \\ eORCA1_MED_UKESM_1m_18500101_18791230_ptrc_T_185011-185011.nc \\ $out_dir/1850/eORCA1_MED_UKESM_y1850m11_ptrc_T.nc 11 ncks --no_abc --cnk_dmn x,64 --cnk_dmn y,64 --cnk_dmn deptht,1 --cnk_dmn time_counter,1 --dfl_lvl 1 eORCA1_MED_UKESM_1m_18500101_18791230_ptrc_T_185012-185012.nc \\ $out_dir/1850/eORCA1_MED_UKESM_y1850m12_ptrc_T.nc Secondly, submit a 'multi_prog' script with appropriate core placement. Here, for example, I have used mkslurm to generate placement for 12 tasks with each task spaced 8 cores apart: #!/bin/bash . #SBATCH --nodes=1 #SBATCH --ntasks=12 # Based on placement generated by: mkslurm -S 0 -s 0 -C 12 -c 8 . srun --ntasks=12 --mem-bind=local \\ --cpu-bind=v,map_cpu:00,0x8,0x10,0x18,0x20,0x28,0x30,0x38,0x40,0x48,0x50,0x58 \\ --multi-prog tasks.conf . This script will process one year of monthly means from the aforementioned configuration in around 2 minutes. Thus, this single-node solution is more than sufficient to keep up with production. The full script, complete with loops and logic to generate and execute a succession of task-lists, can be viewed here .","title":"Running multiple ncks commands simultaneously"},{"location":"postproc/ncks_example/#running-multiple-ncks-commands-simultaneously","text":"For small to medium sized NEMO configurations the 'one_file' option with XIOS is a convenient way to avoid creating too many output files. However, since the XIOS servers use NetCDF's parallel I/O capabilities to write to the single files, this precludes sensible chunking and compression settings. A recommended post-processing step is to chunk and compress the output files to help preserve the consortium's limited disk space. The standard ncks tool will do this well enough but a single batch job may have generated many files that need to be processed. Running the ncks utility serially on the output will take too long and is a very poor use of a 128-core node. Take the example of an eORCA1 run with biogeochemistry which can generate 30 years worth of annual and monthly mean datasets in a 10 hour run. In the case of the monthly mean files there are 9 files per month ranging from small (poleward transport diagnostics) to very large (biogeochemical diagnostics). One sensible way to process these is in a triple-nested loop over year then file-type then month. That way 12, roughly equal files can be processed simultaneously and each member of the 12 should complete is a similar time. Now a single ncks command would be something like: ncks --no_abc --cnk_dmn x,64 --cnk_dmn y,64 --cnk_dmn deptht,1 --cnk_dmn time_counter,1 --dfl_lvl 1 \\ eORCA1_MED_UKESM_1m_18500101_18791230_ptrc_T_185001-185001.nc \\ $out_dir/1850/eORCA1_MED_UKESM_y1850m01_ptrc_T.nc where the opportunity has also been taken to prune redundant parts of the filename to create a more concise output filename. To run 12 copies of a similar command simultaneously on a node requires two simple steps: First, create a task-list file of the 12 commands preceded by a zero-based task number, e.g.: cat tasks.conf 0 ncks --no_abc --cnk_dmn x,64 --cnk_dmn y,64 --cnk_dmn deptht,1 --cnk_dmn time_counter,1 --dfl_lvl 1 \\ eORCA1_MED_UKESM_1m_18500101_18791230_ptrc_T_185001-185001.nc \\ $out_dir/1850/eORCA1_MED_UKESM_y1850m01_ptrc_T.nc 1 ncks --no_abc --cnk_dmn x,64 --cnk_dmn y,64 --cnk_dmn deptht,1 --cnk_dmn time_counter,1 --dfl_lvl 1 \\ eORCA1_MED_UKESM_1m_18500101_18791230_ptrc_T_185002-185002.nc \\ $out_dir/1850/eORCA1_MED_UKESM_y1850m02_ptrc_T.nc 2 ncks --no_abc --cnk_dmn x,64 --cnk_dmn y,64 --cnk_dmn deptht,1 --cnk_dmn time_counter,1 --dfl_lvl 1 \\ eORCA1_MED_UKESM_1m_18500101_18791230_ptrc_T_185003-185003.nc \\ $out_dir/1850/eORCA1_MED_UKESM_y1850m03_ptrc_T.nc . . . 10 ncks --no_abc --cnk_dmn x,64 --cnk_dmn y,64 --cnk_dmn deptht,1 --cnk_dmn time_counter,1 --dfl_lvl 1 \\ eORCA1_MED_UKESM_1m_18500101_18791230_ptrc_T_185011-185011.nc \\ $out_dir/1850/eORCA1_MED_UKESM_y1850m11_ptrc_T.nc 11 ncks --no_abc --cnk_dmn x,64 --cnk_dmn y,64 --cnk_dmn deptht,1 --cnk_dmn time_counter,1 --dfl_lvl 1 eORCA1_MED_UKESM_1m_18500101_18791230_ptrc_T_185012-185012.nc \\ $out_dir/1850/eORCA1_MED_UKESM_y1850m12_ptrc_T.nc Secondly, submit a 'multi_prog' script with appropriate core placement. Here, for example, I have used mkslurm to generate placement for 12 tasks with each task spaced 8 cores apart: #!/bin/bash . #SBATCH --nodes=1 #SBATCH --ntasks=12 # Based on placement generated by: mkslurm -S 0 -s 0 -C 12 -c 8 . srun --ntasks=12 --mem-bind=local \\ --cpu-bind=v,map_cpu:00,0x8,0x10,0x18,0x20,0x28,0x30,0x38,0x40,0x48,0x50,0x58 \\ --multi-prog tasks.conf . This script will process one year of monthly means from the aforementioned configuration in around 2 minutes. Thus, this single-node solution is more than sufficient to keep up with production. The full script, complete with loops and logic to generate and execute a succession of task-lists, can be viewed here .","title":"Running multiple ncks commands simultaneously"},{"location":"postproc/ncks_script/","text":"Full ncks post-processing script #!/bin/bash #SBATCH --qos=standard #SBATCH --job-name=mcnk_1m #SBATCH --time=01:00:00 #SBATCH --nodes=1 #SBATCH --ntasks=12 #SBATCH --account=n01 #SBATCH --partition=standard # Created by: mkslurm -S 0 -s 0 -m 2 -C 12 -c 8 module -s restore /work/n01/shared/acc/n01_modules/ucx_env export OMP_NUM_THREADS=1 export PATH=/work/n01/n01/acc/TOOLS/bin:$PATH export LD_LIBRARY_PATH=/work/n01/n01/acc/TOOLS/lib:$LD_LIBRARY_PATH # dstdir=/work/n01/n01/acc/NEMO/OUT_eORCA1/A006/monthly dtadir=/work/n01/n01/acc/NEMO/r4.0.3/dev_r4.0.3_NERC/cfgs/ORCA1_MEDUSA/EXP_A001/MEANSA6 ##### ystart=2015 yend=2044 ##### tsklst=$dstdir/tasks.conf tskdir=`pwd` # # cd to the data directory # cd $dtadir # # loop over years # for y in `seq $ystart 1 $yend` do # # loop over filetypes # for typ in diaptr2D diaptr3D icemod grid_T grid_U grid_V grid_W diad_T ptrc_T do if [ -f $tsklst ] ; then rm $tsklst ; fi n=0 # # loop over months # for m in 01 02 03 04 05 06 07 08 09 10 11 12 do # # loop over matching files (there should only be a single match) # for f in `ls -1 e*_1m_*${typ}*${y}${m}.nc` do if [ ! -d ${dstdir}/$y ] ; then mkdir ${dstdir}/$y ; fi # # construct a more concise filename # if [ $f == ${f/icemod} ] && [ $f == ${f/diaptr} ]; then ff=`echo $f | sed -e's/\\(.*\\)1m_\\(........_........\\)\\(_[a-zA-Z]*_[a-zA-Z]*\\)_.*-\\(....\\)\\(..\\).nc/\\1y\\4m\\5\\3.nc/'` else ff=`echo $f | sed -e's/\\(.*\\)1m_\\(........_........\\)\\(_[a-zA-Z23]*\\)_.*-\\(....\\)\\(..\\).nc/\\1y\\4m\\5\\3.nc/'` fi if [ ! -f ${dstdir}/$y/$ff ] ; then # # add to the task list # echo $n ncks --no_abc --cnk_dmn x,64 --cnk_dmn y,64 --cnk_dmn deptht,1 --cnk_dmn time_counter,1 --cnk_dmn depthu,1 --cnk_dmn depthv,1 --cnk_dmn depthw,1 --dfl_lvl 1 $f ${dstdir}/$y/$ff >> $tsklst n=$(( $n + 1 )) fi done # end file loop done # end month loop nt=`wc -l $tsklst | sed -e 's/ .*//'` # # if there are tasks, then run them # if test $nt -gt 0 ; then srun --ntasks=$nt --mem-bind=local --cpu-bind=v,map_cpu:00,0x8,0x10,0x18,0x20,0x28,0x30,0x38,0x40,0x48,0x50,0x58 --multi-prog $tsklst fi done # end filetype loop done # end year loop # cd $tskdir","title":"Full ncks post-processing script"},{"location":"postproc/ncks_script/#full-ncks-post-processing-script","text":"#!/bin/bash #SBATCH --qos=standard #SBATCH --job-name=mcnk_1m #SBATCH --time=01:00:00 #SBATCH --nodes=1 #SBATCH --ntasks=12 #SBATCH --account=n01 #SBATCH --partition=standard # Created by: mkslurm -S 0 -s 0 -m 2 -C 12 -c 8 module -s restore /work/n01/shared/acc/n01_modules/ucx_env export OMP_NUM_THREADS=1 export PATH=/work/n01/n01/acc/TOOLS/bin:$PATH export LD_LIBRARY_PATH=/work/n01/n01/acc/TOOLS/lib:$LD_LIBRARY_PATH # dstdir=/work/n01/n01/acc/NEMO/OUT_eORCA1/A006/monthly dtadir=/work/n01/n01/acc/NEMO/r4.0.3/dev_r4.0.3_NERC/cfgs/ORCA1_MEDUSA/EXP_A001/MEANSA6 ##### ystart=2015 yend=2044 ##### tsklst=$dstdir/tasks.conf tskdir=`pwd` # # cd to the data directory # cd $dtadir # # loop over years # for y in `seq $ystart 1 $yend` do # # loop over filetypes # for typ in diaptr2D diaptr3D icemod grid_T grid_U grid_V grid_W diad_T ptrc_T do if [ -f $tsklst ] ; then rm $tsklst ; fi n=0 # # loop over months # for m in 01 02 03 04 05 06 07 08 09 10 11 12 do # # loop over matching files (there should only be a single match) # for f in `ls -1 e*_1m_*${typ}*${y}${m}.nc` do if [ ! -d ${dstdir}/$y ] ; then mkdir ${dstdir}/$y ; fi # # construct a more concise filename # if [ $f == ${f/icemod} ] && [ $f == ${f/diaptr} ]; then ff=`echo $f | sed -e's/\\(.*\\)1m_\\(........_........\\)\\(_[a-zA-Z]*_[a-zA-Z]*\\)_.*-\\(....\\)\\(..\\).nc/\\1y\\4m\\5\\3.nc/'` else ff=`echo $f | sed -e's/\\(.*\\)1m_\\(........_........\\)\\(_[a-zA-Z23]*\\)_.*-\\(....\\)\\(..\\).nc/\\1y\\4m\\5\\3.nc/'` fi if [ ! -f ${dstdir}/$y/$ff ] ; then # # add to the task list # echo $n ncks --no_abc --cnk_dmn x,64 --cnk_dmn y,64 --cnk_dmn deptht,1 --cnk_dmn time_counter,1 --cnk_dmn depthu,1 --cnk_dmn depthv,1 --cnk_dmn depthw,1 --dfl_lvl 1 $f ${dstdir}/$y/$ff >> $tsklst n=$(( $n + 1 )) fi done # end file loop done # end month loop nt=`wc -l $tsklst | sed -e 's/ .*//'` # # if there are tasks, then run them # if test $nt -gt 0 ; then srun --ntasks=$nt --mem-bind=local --cpu-bind=v,map_cpu:00,0x8,0x10,0x18,0x20,0x28,0x30,0x38,0x40,0x48,0x50,0x58 --multi-prog $tsklst fi done # end filetype loop done # end year loop # cd $tskdir","title":"Full ncks post-processing script"},{"location":"postproc/rebuild_nemo_example/","text":"Running multiple rebuild_nemo commands simultaneously For larger configurations the 'one_file' option with XIOS is not necessarily a good choice. Too many XIOS servers trying to write to a single file can produce poor performance. So much so that, with higher frequency output, NEMO can reach the next archive point before the previous has been written. This can lead to a back-log of requests and eventual out-of-memory errors. The larger configurations can also benefit from the NetCDF4 data compression that can only be activated in 'multiple_file' mode. In this case the post-processing task will be to rebuild the multiple strips written by the XIOS servers into single global datasets. There is also some rechunking to do since the default chunksizes set by XIOS are unlikely to be good choices for the global dataset. Consider the example of an eORCA025 configuration with biogeochemistry. A typical setup could employ 16 XIOS servers and output for the biogeochemical mean fields may be: eORCA025_MED_UKESM_1m_18560101_18561230_ptrc_T_185601-185601_0000.nc eORCA025_MED_UKESM_1m_18560101_18561230_ptrc_T_185601-185601_0001.nc . . eORCA025_MED_UKESM_1m_18560101_18561230_ptrc_T_185601-185601_0015.nc . . eORCA025_MED_UKESM_1m_18560101_18561230_ptrc_T_185612-185612_0000.nc eORCA025_MED_UKESM_1m_18560101_18561230_ptrc_T_185612-185612_0001.nc . . eORCA025_MED_UKESM_1m_18560101_18561230_ptrc_T_185612-185612_0015.nc with 12x16 files of monthly means for the 1 year integration. Combining each set of 16 files into a separate monthly mean file can be done with the rebuld_nemo tool. E.g.: tools/REBUILD_NEMO/rebuild_nemo -n reb.nl -p 4 -d 1 -x 64 -y 64 -z 1 -t 1 \\ eORCA025_MED_UKESM_1m_18560101_18561230_ptrc_T_185612-185612 16 But, again, it will be much quicker, and a better use of the nodes, to run multiple versions of such commands simultaneously. This is essentially the same task as the multiple ncks invocations described previously. The main difference being that rebuild_nemo can use OpenMP threads and it may be better to restrict these larger tasks to one per NUMA (16 cores). This requires a 2-node solution to process 12 months at a time. Note also that rebuild_nemo is actually a script that takes the command-line arguments and constructs the named namelist ( not sure why; probably whoever wrote it wasn't aware F90 has standard ways of handling command-line arguments ). This makes little difference other than a need to make sure each innvocation uses a unique name for its namelist; and to tidy up afterwards. The complete solution is shown here but the essence of the controlling slurm script is this: #!/bin/bash . #SBATCH --nodes=2 #SBATCH --ntasks=12 #SBATCH --ntasks-per-node=6 # Based on placement generated by: mkslurm -S 0 -s 0 -C 12 -c 16 export OMP_NUM_THREADS=4 . srun --ntasks=12 --mem-bind=local --cpu-bind=v,map_cpu:00,0x10,0x20,0x30,0x40,0x50 \\ --multi-prog tasks.conf .","title":"Running multiple rebuild_nemo commands simultaneously"},{"location":"postproc/rebuild_nemo_example/#running-multiple-rebuild_nemo-commands-simultaneously","text":"For larger configurations the 'one_file' option with XIOS is not necessarily a good choice. Too many XIOS servers trying to write to a single file can produce poor performance. So much so that, with higher frequency output, NEMO can reach the next archive point before the previous has been written. This can lead to a back-log of requests and eventual out-of-memory errors. The larger configurations can also benefit from the NetCDF4 data compression that can only be activated in 'multiple_file' mode. In this case the post-processing task will be to rebuild the multiple strips written by the XIOS servers into single global datasets. There is also some rechunking to do since the default chunksizes set by XIOS are unlikely to be good choices for the global dataset. Consider the example of an eORCA025 configuration with biogeochemistry. A typical setup could employ 16 XIOS servers and output for the biogeochemical mean fields may be: eORCA025_MED_UKESM_1m_18560101_18561230_ptrc_T_185601-185601_0000.nc eORCA025_MED_UKESM_1m_18560101_18561230_ptrc_T_185601-185601_0001.nc . . eORCA025_MED_UKESM_1m_18560101_18561230_ptrc_T_185601-185601_0015.nc . . eORCA025_MED_UKESM_1m_18560101_18561230_ptrc_T_185612-185612_0000.nc eORCA025_MED_UKESM_1m_18560101_18561230_ptrc_T_185612-185612_0001.nc . . eORCA025_MED_UKESM_1m_18560101_18561230_ptrc_T_185612-185612_0015.nc with 12x16 files of monthly means for the 1 year integration. Combining each set of 16 files into a separate monthly mean file can be done with the rebuld_nemo tool. E.g.: tools/REBUILD_NEMO/rebuild_nemo -n reb.nl -p 4 -d 1 -x 64 -y 64 -z 1 -t 1 \\ eORCA025_MED_UKESM_1m_18560101_18561230_ptrc_T_185612-185612 16 But, again, it will be much quicker, and a better use of the nodes, to run multiple versions of such commands simultaneously. This is essentially the same task as the multiple ncks invocations described previously. The main difference being that rebuild_nemo can use OpenMP threads and it may be better to restrict these larger tasks to one per NUMA (16 cores). This requires a 2-node solution to process 12 months at a time. Note also that rebuild_nemo is actually a script that takes the command-line arguments and constructs the named namelist ( not sure why; probably whoever wrote it wasn't aware F90 has standard ways of handling command-line arguments ). This makes little difference other than a need to make sure each innvocation uses a unique name for its namelist; and to tidy up afterwards. The complete solution is shown here but the essence of the controlling slurm script is this: #!/bin/bash . #SBATCH --nodes=2 #SBATCH --ntasks=12 #SBATCH --ntasks-per-node=6 # Based on placement generated by: mkslurm -S 0 -s 0 -C 12 -c 16 export OMP_NUM_THREADS=4 . srun --ntasks=12 --mem-bind=local --cpu-bind=v,map_cpu:00,0x10,0x20,0x30,0x40,0x50 \\ --multi-prog tasks.conf .","title":"Running multiple rebuild_nemo commands simultaneously"},{"location":"postproc/rebuild_nemo_script/","text":"Example rebuild_nemo submission script #!/bin/bash #SBATCH --qos=standard #SBATCH --job-name=mreb_1m #SBATCH --time=03:00:00 #SBATCH --nodes=2 #SBATCH --ntasks=12 #SBATCH --ntasks-per-node=6 #SBATCH --account=n01 #SBATCH --partition=standard # Based on placement generated by: mkslurm -S 0 -s 0 -C 12 -c 16 # module -s restore /work/n01/shared/acc/n01_modules/ucx_env export PATH=/work/n01/n01/acc/TOOLS/bin:$PATH export LD_LIBRARY_PATH=/work/n01/n01/acc/TOOLS/lib:$LD_LIBRARY_PATH export OMP_NUM_THREADS=4 ################################################################################## # # Parallel rebuild of multiple_file XIOS output from eORCA025 # # This script is configured to rebuild the eORCA025 output generated with 16 # detached XIOS servers. It uses the standard rebuild_nemo tool with 12 instances # running simultaneously across two nodes. Each instance occupies a 16 core NUMA # and uses 4 OpenMP threads. There has been little investigation to find if this # optimal but it works and recombines a years worth of monthly mean output in # around 20 minutes (including some very large biogeochemical datasets) # # The loop order is: year -> filetype -> month so that each invocation of 12 # parallel tasks delivers a years worth of monthly means for that filetype with # each of the 12 tasks having a similar load and completing in roughly the # same length of time # ################################################################################### # # Define the destination directory (annual subdirectories will be created below this) # dstdir=/work/n01/n01/acc/NEMO/OUT_eORCA025/A001/monthly # # Define the data directory containing the distributed mean fields # dtadir=/work/n01/n01/acc/NEMO/r4.0.3/dev_r4.0.3_NERC/cfgs/ORCA025_MEDUSA/EXP00/MEANS_OUT # # Set the number of distributed datasets to combine ( = number of XIOS servers ) # export NUMP=16 # # Provide absolute path to the rebuild_nemo executable/script and a base name for its namelist # reb=/work/n01/n01/acc/NEMO/r4.0.3/dev_r4.0.3_NERC/tools/REBUILD_NEMO/rebuild_nemo nlfile=reb$$.nl # # Set the start and end years of the sequence to process # ystart=1853 yend=1856 # # Define a recyclable file to hold the tasklist and the launch directory # tsklst=$dstdir/tasks.conf tskdir=`pwd` # cd $dtadir # # Loop through each year # for y in `seq $ystart 1 $yend` do # # Loop through each type of dataset (expand or contract as required) # for typ in icemod grid_T grid_U grid_V grid_W diad_T ptrc_T do if [ -f $tsklst ] ; then rm $tsklst ; fi n=0 # # Loop through each month (do not extend beyond 12) # for m in 01 02 03 04 05 06 07 08 09 10 11 12 do # # find the basename for this year/type/month combination # (there should only be one match - do not really need a loop here) # for fin in `ls -1 e*_1m_*${typ}*${y}${m}_0000.nc` do f=`echo $fin | sed -e's/_0000.nc//'` if [ ! -d ${dstdir}/$y ] ; then mkdir ${dstdir}/$y ; fi # # Construct a more concise output filename # if [ $f == ${f/icemod} ] && [ $f == ${f/diaptr} ]; then ff=`echo $f | sed -e's/\\(.*\\)1m_\\(........_........\\)\\(_[a-zA-Z]*_[a-zA-Z]*\\)_.*-\\(....\\)\\(..\\)/\\1y\\4m\\5\\3.nc/'` else ff=`echo $f | sed -e's/\\(.*\\)1m_\\(........_........\\)\\(_[a-zA-Z23]*\\)_.*-\\(....\\)\\(..\\)/\\1y\\4m\\5\\3.nc/'` fi # # If to be processed, then append the full rebuild command to the task list # if [ ! -f ${dstdir}/$y/$ff ] ; then echo $n $reb -n ${n}_$nlfile -p $OMP_NUM_THREADS -d 1 -x 64 -y 64 -z 1 -t 1 $f $NUMP >> $tsklst # # rebuild_nemo doe not allow us to set the output filename. Store what is used, what we want it # to be (including final destination) and the namelist file (so that it can be tidied later) # fcomb[$n]=$f.nc frenm[$n]=${dstdir}/$y/$ff fnlfi[$n]=${n}_$nlfile n=$(( $n + 1 )) fi done # # End of scan through months # done # # A tasklist of up to 12 tasks has been constructed; check its length # if [ -f $tsklst ] ; then nt=`wc -l $tsklst | sed -e 's/ .*//'` else nt=0 fi # # Run any non-zero length tasklist using srun and appropriate placement # if test $nt -gt 0 ; then srun --ntasks=$nt --mem-bind=local --cpu-bind=v,map_cpu:00,0x10,0x20,0x30,0x40,0x50 --multi-prog $tsklst # # On completion the combined files have the wrong names and are still in the data directory # Use the stored arrays to move/rename them and remove the temporary namelist files # for k in `seq 0 1 $(( $nt - 1 ))` do if [ -f ${fcomb[$k]} ] ; then mv ${fcomb[$k]} ${frenm[$k]} ; fi if [ -f ${fnlfi[$k]} ] ; then rm ${fnlfi[$k]} ; fi done fi # # End of file-type loop # done # # End of year loop # done # cd $tskdir","title":"Example rebuild_nemo submission script"},{"location":"postproc/rebuild_nemo_script/#example-rebuild_nemo-submission-script","text":"#!/bin/bash #SBATCH --qos=standard #SBATCH --job-name=mreb_1m #SBATCH --time=03:00:00 #SBATCH --nodes=2 #SBATCH --ntasks=12 #SBATCH --ntasks-per-node=6 #SBATCH --account=n01 #SBATCH --partition=standard # Based on placement generated by: mkslurm -S 0 -s 0 -C 12 -c 16 # module -s restore /work/n01/shared/acc/n01_modules/ucx_env export PATH=/work/n01/n01/acc/TOOLS/bin:$PATH export LD_LIBRARY_PATH=/work/n01/n01/acc/TOOLS/lib:$LD_LIBRARY_PATH export OMP_NUM_THREADS=4 ################################################################################## # # Parallel rebuild of multiple_file XIOS output from eORCA025 # # This script is configured to rebuild the eORCA025 output generated with 16 # detached XIOS servers. It uses the standard rebuild_nemo tool with 12 instances # running simultaneously across two nodes. Each instance occupies a 16 core NUMA # and uses 4 OpenMP threads. There has been little investigation to find if this # optimal but it works and recombines a years worth of monthly mean output in # around 20 minutes (including some very large biogeochemical datasets) # # The loop order is: year -> filetype -> month so that each invocation of 12 # parallel tasks delivers a years worth of monthly means for that filetype with # each of the 12 tasks having a similar load and completing in roughly the # same length of time # ################################################################################### # # Define the destination directory (annual subdirectories will be created below this) # dstdir=/work/n01/n01/acc/NEMO/OUT_eORCA025/A001/monthly # # Define the data directory containing the distributed mean fields # dtadir=/work/n01/n01/acc/NEMO/r4.0.3/dev_r4.0.3_NERC/cfgs/ORCA025_MEDUSA/EXP00/MEANS_OUT # # Set the number of distributed datasets to combine ( = number of XIOS servers ) # export NUMP=16 # # Provide absolute path to the rebuild_nemo executable/script and a base name for its namelist # reb=/work/n01/n01/acc/NEMO/r4.0.3/dev_r4.0.3_NERC/tools/REBUILD_NEMO/rebuild_nemo nlfile=reb$$.nl # # Set the start and end years of the sequence to process # ystart=1853 yend=1856 # # Define a recyclable file to hold the tasklist and the launch directory # tsklst=$dstdir/tasks.conf tskdir=`pwd` # cd $dtadir # # Loop through each year # for y in `seq $ystart 1 $yend` do # # Loop through each type of dataset (expand or contract as required) # for typ in icemod grid_T grid_U grid_V grid_W diad_T ptrc_T do if [ -f $tsklst ] ; then rm $tsklst ; fi n=0 # # Loop through each month (do not extend beyond 12) # for m in 01 02 03 04 05 06 07 08 09 10 11 12 do # # find the basename for this year/type/month combination # (there should only be one match - do not really need a loop here) # for fin in `ls -1 e*_1m_*${typ}*${y}${m}_0000.nc` do f=`echo $fin | sed -e's/_0000.nc//'` if [ ! -d ${dstdir}/$y ] ; then mkdir ${dstdir}/$y ; fi # # Construct a more concise output filename # if [ $f == ${f/icemod} ] && [ $f == ${f/diaptr} ]; then ff=`echo $f | sed -e's/\\(.*\\)1m_\\(........_........\\)\\(_[a-zA-Z]*_[a-zA-Z]*\\)_.*-\\(....\\)\\(..\\)/\\1y\\4m\\5\\3.nc/'` else ff=`echo $f | sed -e's/\\(.*\\)1m_\\(........_........\\)\\(_[a-zA-Z23]*\\)_.*-\\(....\\)\\(..\\)/\\1y\\4m\\5\\3.nc/'` fi # # If to be processed, then append the full rebuild command to the task list # if [ ! -f ${dstdir}/$y/$ff ] ; then echo $n $reb -n ${n}_$nlfile -p $OMP_NUM_THREADS -d 1 -x 64 -y 64 -z 1 -t 1 $f $NUMP >> $tsklst # # rebuild_nemo doe not allow us to set the output filename. Store what is used, what we want it # to be (including final destination) and the namelist file (so that it can be tidied later) # fcomb[$n]=$f.nc frenm[$n]=${dstdir}/$y/$ff fnlfi[$n]=${n}_$nlfile n=$(( $n + 1 )) fi done # # End of scan through months # done # # A tasklist of up to 12 tasks has been constructed; check its length # if [ -f $tsklst ] ; then nt=`wc -l $tsklst | sed -e 's/ .*//'` else nt=0 fi # # Run any non-zero length tasklist using srun and appropriate placement # if test $nt -gt 0 ; then srun --ntasks=$nt --mem-bind=local --cpu-bind=v,map_cpu:00,0x10,0x20,0x30,0x40,0x50 --multi-prog $tsklst # # On completion the combined files have the wrong names and are still in the data directory # Use the stored arrays to move/rename them and remove the temporary namelist files # for k in `seq 0 1 $(( $nt - 1 ))` do if [ -f ${fcomb[$k]} ] ; then mv ${fcomb[$k]} ${frenm[$k]} ; fi if [ -f ${fnlfi[$k]} ] ; then rm ${fnlfi[$k]} ; fi done fi # # End of file-type loop # done # # End of year loop # done # cd $tskdir","title":"Example rebuild_nemo submission script"},{"location":"postproc/repro_example/","text":"Running multiple, small-scale parallel tasks Another scenario that requires special consideration on ARCHER2 is a need to run multiple, small-scale parallel tasks. This may be for ensembles of perturbed models or for investigations into the effect of varying parameter values. If each model only requires a fraction of an ARCHER2 node then a method of launching multiple instances within each node is required. Slurm permits this by allowing a series of srun commands to be launched within a job script with each subsequent command backgrounded (i.e. with the usual Unix protocol of appending an ampersand). This will work so long as the overall resource requirement does not exceed that requested by the job. However, the unhelpful, default behaviour on ARCHER2 is to launch each srun command on the same cores so explicit placement needs to be generated for each srun command to achieve an effective result. It is also usual to prepare experiment directories for each invocation in advance and to run each srun command in a different experiment directory. The following example illustrates such a scenario: #!/bin/bash #SBATCH --qos=standard #SBATCH --job-name=nemo_test #SBATCH --time=00:10:00 #SBATCH --nodes=1 #SBATCH --ntasks=16 #SBATCH --account=n01 #SBATCH --partition=standard module -s restore /work/n01/shared/acc/n01_modules/ucx_env export OMP_NUM_THREADS=1 # cd /work/n01/n01/acc/NEMO/2021/dev_r14312_MPI_Interface/cfgs/GYRE_PISCES_ST/REPRO_4_2 srun --ntasks=8 --mem-bind=local --cpu-bind=v,map_cpu:00,0x1,0x2,0x3,0x4,0x5,0x6,0x7, ./nemo & # cd /work/n01/n01/acc/NEMO/2021/dev_r14312_MPI_Interface/cfgs/GYRE_PISCES_ST/REPRO_2_4 srun --ntasks=8 --mem-bind=local --cpu-bind=v,map_cpu:0x10,0x11,0x12,0x13,0x14,0x15,0x16,0x17, ./nemo & wait where two, 8-core GYRE_PISCES tests have been run simultaneously on a single node with each correctly placed in one of the first two 16-core NUMA regions. Note also the inclusion of the wait command after the last srun. This prevents the job-shell from exiting until all the backgrounded tasks have completed. Clearly, generating such scripts for many-member ensembles would be tedious. Fortunately, an extension of the mkslurm_hetjob script (mkslurm_hetjob_ensemble) has been constructed (thanks Mattia!) to include this capability. This allows all the same placement arguments as the standard scripts but also accepts a series of named directories supplied to the -D argument. A similarly distributed task is produced for each directory listed with a cd to the appropriate directory between each srun command. Thus, an improvement on the previous example, this time including the use of XIOS servers with each task, can be generated by: dir1=/work/n01/n01/acc/NEMO/2021/dev_r14312_MPI_Interface/cfgs/GYRE_PISCES_ST/REPRO_4_2 dir2=/work/n01/n01/acc/NEMO/2021/dev_r14312_MPI_Interface/cfgs/GYRE_PISCES_ST/REPRO_2_4 /work/n01/shared/malmans/mkslurm_hetjob_ensemble -S 1 -s 8 -C 8 -g 0 m 2 -D $dir1 $dir2 which produces this script ready for submission: #!/bin/bash #SBATCH --job-name=nemo_test #SBATCH --time=00:10:00 #SBATCH --account=n01 #SBATCH --qos=standard #SBATCH --partition=standard #SBATCH --nodes=1 #SBATCH --ntasks=18 #SBATCH --ntasks-per-node=18 #SBATCH --ntasks-per-core=1 # Created by: mkslurm_hetjob_ensemble -S 1 -s 8 -m 2 -C 8 -g 0 -D ['/work/n01/n01/acc/NEMO/2021/dev_r14312_MPI_Interface/cfgs/GYRE_PISCES_ST/REPRO_4_2', '/work/n01/n01/acc/NEMO/2021/dev_r14312_MPI_Interface/cfgs/GYRE_PISCES_ST/REPRO_2_4'] --alternate-dirs False -N 128 -t 00:10:00 -a n01 -j nemo_test -q standard -v False module -s restore /work/n01/shared/acc/n01_modules/ucx_env export OMP_NUM_THREADS=1 # Ensemble 0: /work/n01/n01/acc/NEMO/2021/dev_r14312_MPI_Interface/cfgs/GYRE_PISCES_ST/REPRO_4_2 cat > \"$SLURM_SUBMIT_DIR\"/myscript_wrapper_0.sh << EOFB #!/bin/ksh # set -A map ./xios_server.exe ./nemo exec_map=( 0 1 1 1 1 1 1 1 1 ) # exec \\${map[\\${exec_map[\\$SLURM_PROCID]}]} ## EOFB chmod u+x \"$SLURM_SUBMIT_DIR\"/myscript_wrapper_0.sh cd /work/n01/n01/acc/NEMO/2021/dev_r14312_MPI_Interface/cfgs/GYRE_PISCES_ST/REPRO_4_2 || exit srun --mem-bind=local \\ --ntasks=9 --ntasks-per-node=9 --cpu-bind=v,mask_cpu:0x1,0x10000,0x20000,0x40000,0x80000,0x100000,0x200000,0x400000,0x800000 \"$SLURM_SUBMIT_DIR\"/myscript_wrapper_0.sh & cd \"$SLURM_SUBMIT_DIR\" || exit # Ensemble 1: /work/n01/n01/acc/NEMO/2021/dev_r14312_MPI_Interface/cfgs/GYRE_PISCES_ST/REPRO_2_4 cat > \"$SLURM_SUBMIT_DIR\"/myscript_wrapper_1.sh << EOFB #!/bin/ksh # set -A map ./xios_server.exe ./nemo exec_map=( 0 1 1 1 1 1 1 1 1 ) # exec \\${map[\\${exec_map[\\$SLURM_PROCID]}]} ## EOFB chmod u+x \"$SLURM_SUBMIT_DIR\"/myscript_wrapper_1.sh cd /work/n01/n01/acc/NEMO/2021/dev_r14312_MPI_Interface/cfgs/GYRE_PISCES_ST/REPRO_2_4 || exit srun --mem-bind=local \\ --ntasks=9 --ntasks-per-node=9 --cpu-bind=v,mask_cpu:0x100,0x1000000,0x2000000,0x4000000,0x8000000,0x10000000,0x20000000,0x40000000,0x80000000 \"$SLURM_SUBMIT_DIR\"/myscript_wrapper_1.sh & cd \"$SLURM_SUBMIT_DIR\" || exit wait Note this script will correctly extend across multiple nodes if required and separate into hetjob components if there is non-uniform use of the nodes.","title":"Running multiple, small-scale parallel tasks"},{"location":"postproc/repro_example/#running-multiple-small-scale-parallel-tasks","text":"Another scenario that requires special consideration on ARCHER2 is a need to run multiple, small-scale parallel tasks. This may be for ensembles of perturbed models or for investigations into the effect of varying parameter values. If each model only requires a fraction of an ARCHER2 node then a method of launching multiple instances within each node is required. Slurm permits this by allowing a series of srun commands to be launched within a job script with each subsequent command backgrounded (i.e. with the usual Unix protocol of appending an ampersand). This will work so long as the overall resource requirement does not exceed that requested by the job. However, the unhelpful, default behaviour on ARCHER2 is to launch each srun command on the same cores so explicit placement needs to be generated for each srun command to achieve an effective result. It is also usual to prepare experiment directories for each invocation in advance and to run each srun command in a different experiment directory. The following example illustrates such a scenario: #!/bin/bash #SBATCH --qos=standard #SBATCH --job-name=nemo_test #SBATCH --time=00:10:00 #SBATCH --nodes=1 #SBATCH --ntasks=16 #SBATCH --account=n01 #SBATCH --partition=standard module -s restore /work/n01/shared/acc/n01_modules/ucx_env export OMP_NUM_THREADS=1 # cd /work/n01/n01/acc/NEMO/2021/dev_r14312_MPI_Interface/cfgs/GYRE_PISCES_ST/REPRO_4_2 srun --ntasks=8 --mem-bind=local --cpu-bind=v,map_cpu:00,0x1,0x2,0x3,0x4,0x5,0x6,0x7, ./nemo & # cd /work/n01/n01/acc/NEMO/2021/dev_r14312_MPI_Interface/cfgs/GYRE_PISCES_ST/REPRO_2_4 srun --ntasks=8 --mem-bind=local --cpu-bind=v,map_cpu:0x10,0x11,0x12,0x13,0x14,0x15,0x16,0x17, ./nemo & wait where two, 8-core GYRE_PISCES tests have been run simultaneously on a single node with each correctly placed in one of the first two 16-core NUMA regions. Note also the inclusion of the wait command after the last srun. This prevents the job-shell from exiting until all the backgrounded tasks have completed. Clearly, generating such scripts for many-member ensembles would be tedious. Fortunately, an extension of the mkslurm_hetjob script (mkslurm_hetjob_ensemble) has been constructed (thanks Mattia!) to include this capability. This allows all the same placement arguments as the standard scripts but also accepts a series of named directories supplied to the -D argument. A similarly distributed task is produced for each directory listed with a cd to the appropriate directory between each srun command. Thus, an improvement on the previous example, this time including the use of XIOS servers with each task, can be generated by: dir1=/work/n01/n01/acc/NEMO/2021/dev_r14312_MPI_Interface/cfgs/GYRE_PISCES_ST/REPRO_4_2 dir2=/work/n01/n01/acc/NEMO/2021/dev_r14312_MPI_Interface/cfgs/GYRE_PISCES_ST/REPRO_2_4 /work/n01/shared/malmans/mkslurm_hetjob_ensemble -S 1 -s 8 -C 8 -g 0 m 2 -D $dir1 $dir2 which produces this script ready for submission: #!/bin/bash #SBATCH --job-name=nemo_test #SBATCH --time=00:10:00 #SBATCH --account=n01 #SBATCH --qos=standard #SBATCH --partition=standard #SBATCH --nodes=1 #SBATCH --ntasks=18 #SBATCH --ntasks-per-node=18 #SBATCH --ntasks-per-core=1 # Created by: mkslurm_hetjob_ensemble -S 1 -s 8 -m 2 -C 8 -g 0 -D ['/work/n01/n01/acc/NEMO/2021/dev_r14312_MPI_Interface/cfgs/GYRE_PISCES_ST/REPRO_4_2', '/work/n01/n01/acc/NEMO/2021/dev_r14312_MPI_Interface/cfgs/GYRE_PISCES_ST/REPRO_2_4'] --alternate-dirs False -N 128 -t 00:10:00 -a n01 -j nemo_test -q standard -v False module -s restore /work/n01/shared/acc/n01_modules/ucx_env export OMP_NUM_THREADS=1 # Ensemble 0: /work/n01/n01/acc/NEMO/2021/dev_r14312_MPI_Interface/cfgs/GYRE_PISCES_ST/REPRO_4_2 cat > \"$SLURM_SUBMIT_DIR\"/myscript_wrapper_0.sh << EOFB #!/bin/ksh # set -A map ./xios_server.exe ./nemo exec_map=( 0 1 1 1 1 1 1 1 1 ) # exec \\${map[\\${exec_map[\\$SLURM_PROCID]}]} ## EOFB chmod u+x \"$SLURM_SUBMIT_DIR\"/myscript_wrapper_0.sh cd /work/n01/n01/acc/NEMO/2021/dev_r14312_MPI_Interface/cfgs/GYRE_PISCES_ST/REPRO_4_2 || exit srun --mem-bind=local \\ --ntasks=9 --ntasks-per-node=9 --cpu-bind=v,mask_cpu:0x1,0x10000,0x20000,0x40000,0x80000,0x100000,0x200000,0x400000,0x800000 \"$SLURM_SUBMIT_DIR\"/myscript_wrapper_0.sh & cd \"$SLURM_SUBMIT_DIR\" || exit # Ensemble 1: /work/n01/n01/acc/NEMO/2021/dev_r14312_MPI_Interface/cfgs/GYRE_PISCES_ST/REPRO_2_4 cat > \"$SLURM_SUBMIT_DIR\"/myscript_wrapper_1.sh << EOFB #!/bin/ksh # set -A map ./xios_server.exe ./nemo exec_map=( 0 1 1 1 1 1 1 1 1 ) # exec \\${map[\\${exec_map[\\$SLURM_PROCID]}]} ## EOFB chmod u+x \"$SLURM_SUBMIT_DIR\"/myscript_wrapper_1.sh cd /work/n01/n01/acc/NEMO/2021/dev_r14312_MPI_Interface/cfgs/GYRE_PISCES_ST/REPRO_2_4 || exit srun --mem-bind=local \\ --ntasks=9 --ntasks-per-node=9 --cpu-bind=v,mask_cpu:0x100,0x1000000,0x2000000,0x4000000,0x8000000,0x10000000,0x20000000,0x40000000,0x80000000 \"$SLURM_SUBMIT_DIR\"/myscript_wrapper_1.sh & cd \"$SLURM_SUBMIT_DIR\" || exit wait Note this script will correctly extend across multiple nodes if required and separate into hetjob components if there is non-uniform use of the nodes.","title":"Running multiple, small-scale parallel tasks"}]}